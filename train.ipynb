{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6d5797-3980-4590-a9f5-08d1a1c3c858",
   "metadata": {},
   "source": [
    "## Loading librarires and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72f98ac-ec4d-4560-a755-959a4312c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/miniconda3/envs/cuda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import argparse\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa823451-8ef3-4267-8207-abed95e01864",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca68adf-5841-4bd8-87e5-617ab01828fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './dataset/'\n",
    "df = pd.read_csv(data_dir+'processed_train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40040166-5f8f-47db-8584-3909ef11bc27",
   "metadata": {},
   "source": [
    "* use 2000 samples to make sure my code run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa674466-b18d-47fc-8e72-addc9b47f93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hannibal  \\n\\nLike I said earlier, the only th...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\n some old news about second expansion \\n\\...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rude C*nt \\n\\nDon't delete the streak on Jack ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The standard offer \\n\\nUser:Tokyogirl79\\nWhat ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\\n\\nI think it should be removed, yes. Could ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text              labels\n",
       "0  Hannibal  \\n\\nLike I said earlier, the only th...  [0, 0, 0, 0, 0, 0]\n",
       "1  \"\\n\\n some old news about second expansion \\n\\...  [0, 0, 0, 0, 0, 0]\n",
       "2  Rude C*nt \\n\\nDon't delete the streak on Jack ...  [1, 0, 0, 0, 0, 0]\n",
       "3  The standard offer \\n\\nUser:Tokyogirl79\\nWhat ...  [0, 0, 0, 0, 0, 0]\n",
       "4  \"\\n\\nI think it should be removed, yes. Could ...  [0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = df.sample(2000).reset_index(drop=True)\n",
    "dfs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5dea6e-e14d-4a18-88df-7bd15dcc65c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\n some old news about second expansion \\n\\n2nd expansion, Experimentals, should be called \"\"The Experimentals\"\"? at least CT mentioned like that and in official GPG forum we all call it TE, so i guess better keep the word \"\"The\"\".\\nAnother thing, TE now canceled, as seen in recent(not really, some mth ago) interview, CT talked about this. Can check supcom fan site for this news or forum old topic.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.comment_text.values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a7ecb-030c-471a-983b-c77f9e0fe8cb",
   "metadata": {},
   "source": [
    "Split into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa2a462-4f99-41f2-a192-b1c6eb68a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dfs.sample(frac=0.8, random_state=42)\n",
    "val_df = dfs.drop(train_df.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2507b-99c5-4d25-a9d2-ff6a87092298",
   "metadata": {},
   "source": [
    "## Creating Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d6fa68-55e3-42ee-99a3-1b85b091f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tokenizer.encode_plus` not found.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.encode_plus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d40ac05-b312-4956-888b-a3a45c1b7e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer: transformers.BertTokenizer, max_len = 128):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.text = self.df.comment_text.values\n",
    "        self.label = self.df.labels.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        input = self.tokenizer.encode_plus(\n",
    "            text = text,\n",
    "            text_pair = None,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length = self.max_len,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids' : input.input_ids.squeeze(),\n",
    "            'attention_mask' : input.attention_mask.squeeze(), \n",
    "            'token_type_ids' : input.token_type_ids.squeeze()\n",
    "        }\n",
    "# dataset = MyDataset(dfs, tokenizer, 100) # 512 is tokenizer.model_max_length\n",
    "# dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bfd869-adfc-4efe-9a5d-f652cbdb755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./bert')\n",
    "train = MyDataset(train_df, tokenizer, 512)\n",
    "val = MyDataset(val_df, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d496ce5-0d2b-421f-aa58-89711a8109ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1000,  1024,  1024,  1024,  2054,  1005,  1055, 26316,  2003,\n",
       "          2008,  1996,  1000,  1000,  3296,  1000,  1000, 10086,  2038,  2468,\n",
       "          1037,  4568, 15083,  1999,  5606,  1997, 11744,  1012,  2074,  2079,\n",
       "          1037,  3945,  2006,  1000,  1000,  7955,  4009, 12619,  2154,  2003,\n",
       "          2019,  3296,  2724,  1012,  1000,  1000,  2017,  1005,  2222,  2131,\n",
       "          2055,  1015,  1010,  2199,  4978,  1010,  1998,  2471,  2035,  2024,\n",
       "         27394, 16948,   999,   999,  1011,  1007,  2026,  5440,  2003,  1037,\n",
       "          2678,  2011,  8505,  2546,  8889,  2102,  1012,  2175,  2000,  1017,\n",
       "          1024,  2539,  1997,  1000,  1000,  5255,  2431,  1037,  2454, 12849,\n",
       "          5521,  2015,  1012,  1000,  1000,  2009,  1005,  1055,  1037, 12117,\n",
       "         12326,  1997, 16948,  4760,  1996,  3296, 12034,  2401,  3351,  1012,\n",
       "          2293,  2009,   999,  1011,  1007,  1000,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd01eba-aff5-4899-aee7-279132e7834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f2c511-a37a-424e-8cef-482c6c238b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfbe5c-1594-4560-98c9-e0deddb71f5c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d72728e8-c7f3-4ace-bd7e-d397e195c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef9b12d-0b2c-45dc-a240-5ef182b64a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(params=model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049348d3-9a4c-4c82-aac5-183947b6de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('./bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef67815-5726-4427-8829-f42b57128fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62404841-1cf1-45ce-aa24-70dae99191d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2123, 1005,  ...,    0,    0,    0],\n",
       "        [ 101, 1045, 9530,  ...,    0,    0,    0],\n",
       "        [ 101, 2045, 2052,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1000, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 1000, 1045,  ...,    0,    0,    0],\n",
       "        [ 101, 2092, 1045,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be0d30-c7aa-40d4-b8a0-efe1282672cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(batch['input_ids'],batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3aa1d-7666-46ae-9c3f-3b080e4b6bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
